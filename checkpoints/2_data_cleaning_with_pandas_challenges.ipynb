{"cells":[{"cell_type":"markdown","metadata":{"id":"kYo4UvcCvkUs"},"source":["# Data Cleaning with Pandas\n","\n","In this notebook we'll go through a few basic data cleaning steps that should be performed on all new datasets where necessary.\n","\n","We'll go through the process with both the `orders` and `orderlines` datasets. You can then practice these skills by cleaning the `products` dataset yourself"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgRIdUs1vcZ8"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_87JoNgVviOG"},"outputs":[],"source":["# orders.csv\n","url = \"https://drive.google.com/file/d/1Vu0q91qZw6lqhIqbjoXYvYAQTmVHh6uZ/view?usp=sharing\"\n","path = \"https://drive.google.com/uc?export=download&id=\"+url.split(\"/\")[-2]\n","orders = pd.read_csv(path)\n","\n","# orderlines.csv\n","url = \"https://drive.google.com/file/d/1FYhN_2AzTBFuWcfHaRuKcuCE6CWXsWtG/view?usp=sharing\"\n","path = \"https://drive.google.com/uc?export=download&id=\"+url.split(\"/\")[-2]\n","orderlines = pd.read_csv(path)"]},{"cell_type":"markdown","metadata":{"id":"esw86QhgnPXr"},"source":["Before we begin, let's create a copy of the `orders` and `orderlines` DataFrames. This way we are sure any of our changes won't affect the original DataFrames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7CSWUD4nPXr"},"outputs":[],"source":["orders_df = orders.copy()"]},{"cell_type":"code","source":["orderlines_df = orderlines.copy()"],"metadata":{"id":"qHu5kTfCKnIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fVLitYX1yhc6"},"source":["One of the best ways to begin data cleaning is by exploring using `.info()`. This will tell us:\n","* The shape of the DataFrame\n","* The names of the columns\n","* If there are any missing values\n","* The datatypes of the columns\n","\n","By exploring the missing values and correcting any incorrect datatypes, we often come across inconsistencies in our data.\n","\n","Beyond this, we should also have a **check for any duplicate rows**.\n","\n","Let's first deal with the duplicates, as it's nice and easy, then we'll explore what `.info()` has to tell us."]},{"cell_type":"markdown","metadata":{"id":"036ciQ6rwBJm"},"source":["## 1.&nbsp; Duplicates\n","We can check for duplicates using the pandas [.duplicated()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html) method.\n","\n","We can then delete these rows, if we wish, using [.drop_duplicates()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dVkoUu7wC-b"},"outputs":[],"source":["# orders_df\n","orders_df.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaeAQB_3whli"},"outputs":[],"source":["# orderlines_df\n","orderlines_df.duplicated().sum()"]},{"cell_type":"code","source":["orders_df.drop_duplicates()"],"metadata":{"id":"sh_ys9f889tq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3LbnJquF0vRe"},"source":["We have no duplicate rows in either DataFrame. Easy, there is no problem to solve. Normally though, if there were some duplicates, we'd drop the extra rows."]},{"cell_type":"markdown","metadata":{"id":"9lX7YbHlzoEO"},"source":["# 2.&nbsp; `.info()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AAsMOTS2L9r"},"outputs":[],"source":["orders_df.info()"]},{"cell_type":"markdown","metadata":{"id":"Txm3Bv952UCb"},"source":["* `total_paid` has 5 missing values\n","* `created_date` should become datetime datatype"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvDA-z032Rm2"},"outputs":[],"source":["orderlines_df.info()"]},{"cell_type":"markdown","metadata":{"id":"iLSpYm1G2sPD"},"source":["* `date` should be a datetime datatype\n","* `unit_price` should be a float datatype"]},{"cell_type":"markdown","metadata":{"id":"MlwJt5c-wiQh"},"source":["## 3.&nbsp; Missing values"]},{"cell_type":"markdown","metadata":{"id":"63eKHGzJwkuT"},"source":["### 3.1.&nbsp; Orders\n","* `total_paid` has 5 missing values"]},{"cell_type":"code","source":["# Clean for each calculation\n","# Pro: Uses as much data as possible for each calculation\n","\n","\n","# Clean, then calculate\n","# Pro: Consistent"],"metadata":{"id":"_nZSfprA9tz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["orders_df.isna().sum()"],"metadata":{"id":"_uZ9hKKC_Sw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["orders_df['total_paid'].isna().sum()"],"metadata":{"id":"Iu_bazywALvf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yh81k3ppwl37"},"outputs":[],"source":["num_missing = orders_df['total_paid'].isna().sum()\n","total_rows = orders_df.shape[0]\n","percent_missing = (100*num_missing/total_rows)\n","print(f\"5 missing values represents {percent_missing:.5f}% of the rows in our DataFrame\")"]},{"cell_type":"markdown","metadata":{"id":"8B7GYG3ZZWC1"},"source":["> A quick way to find out a percentage if you don't need to print out a sentence for yourself/students/colleagues is `.value_count(normalize=True)`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjkDrufyZZQi"},"outputs":[],"source":["orders_df['total_paid'].isna().value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{"id":"eMW6J-4v_iVM"},"source":["As there is such a tiny amount of missing values, we will simply delete these rows, as we have enough data without them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohcl2Pb2B_Ch"},"outputs":[],"source":["orders_df = orders_df.dropna(axis=0)"]},{"cell_type":"markdown","metadata":{"id":"AIWAR4xI_3jv"},"source":["Should you have a significant number of missing values in the future, you have a choice:\n","+ you can impute the values\n","+ you can take the values from other DataFrames if they are redundantly stored\n","+ you can delete the rows or columns\n","+ or any number of other creative solutions\n","\n","Please, always consider how much time you have on your project, and what impact your method of choice will have on your final assesment."]},{"cell_type":"markdown","metadata":{"id":"nziIE3hbwmNa"},"source":["### 3.2.&nbsp; Orderlines\n","There are no missing values in `orderlines_df`"]},{"cell_type":"markdown","metadata":{"id":"_feog1aCwosJ"},"source":["## 4.&nbsp; Datatypes"]},{"cell_type":"markdown","metadata":{"id":"wCJyQYkOwrBE"},"source":["### 4.1.&nbsp; Orders\n","* `created_date` should become datetime datatype"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cF9roGlewr8j"},"outputs":[],"source":["orders_df[\"created_date\"] = pd.to_datetime(orders_df[\"created_date\"])"]},{"cell_type":"markdown","metadata":{"id":"osvynBTNwsin"},"source":["### 4.1.&nbsp; Orderlines\n","* `date` should be a datetime datatype\n","* `unit_price` should be a float datatype"]},{"cell_type":"markdown","metadata":{"id":"PJcuKn8K06Yd"},"source":["#### 4.1.1.&nbsp; `date`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnXgRKERwtvS"},"outputs":[],"source":["orderlines_df[\"date\"] = pd.to_datetime(orderlines_df[\"date\"])"]},{"cell_type":"markdown","metadata":{"id":"F7i78NDY0-oF"},"source":["#### 4.1.2.&nbsp;`unit_price`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOFBNmuRDuGU"},"outputs":[],"source":["orderlines_df[\"unit_price\"] = pd.to_numeric(orderlines_df[\"unit_price\"])"]},{"cell_type":"markdown","metadata":{"id":"xEqhP5RFSmRi"},"source":["As you can see when we try to convert `unit_price` to a numerical datatype, we receive a `ValueError` telling us that pandas doesn't understand the number `1.137.99`. This is probably because numbers cannot have multiple decimal points. Let's see if there are any other numbers like this:\n","\n","> `.` is a wildcard in regex, we need the `\\` as an escape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONy-oCt0SkrR"},"outputs":[],"source":["# Count the number of decimal points in the unit_price\n","orderlines_df['unit_price'].str.count(\"\\.\").value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{"id":"HSUPvFJU6Jv-"},"source":["Looks like over 36000 rows in `orderlines` are affected by this problem. Let's work out how much that is as a percentage of our total data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r75Rm1EgVjpi"},"outputs":[],"source":["# Count the rows with more than one `.`\n","mult_decimal_rows = (orderlines_df['unit_price'].str.count(\"\\.\")>1).sum()\n","\n","# Find the percentage of corrupted rows\n","percent_corrupted = (100 * mult_decimal_rows / orderlines_df.shape[0])\n","print(f\"{percent_corrupted:.2f}% of the rows in our DataFrame have multiple decimal points in the unit_price\")"]},{"cell_type":"markdown","metadata":{"id":"5Kp9_4pqeu5J"},"source":["This is a bit of a tricky decision as 12.3% is a significant amount of our data... and we might even end up losing a larger portion of our data than this too. For the moment we will delete the rows as we only have 2 weeks for this project and I'd like some quick, accurate results to show. If we have time at the end, we can come back and investigate this problem further, maybe there's a solution?\n","\n","Each row of `orderlines` represents a product in an order. For example, if order number 175 contained 3 seperate products, then order 175 would have 3 rows in `orderlines`, one row for each of the products. If 2 of those products have 'normal' prices (14.99, 15.85) and 1 has a price with 2 decimal points (1.137.99), we need to remove the whole order and not just the affected row. If we only remove the row with 2 decimal places then any later analysis about products and prices could be misleading.\n","\n","We therefore need to find the order numbers associated with the rows that have 2 decimal points, and then remove all the associated rows."]},{"cell_type":"code","source":["orderlines_df.loc[orderlines_df['unit_price'].str.count(\"\\.\") > 1]"],"metadata":{"id":"gpxOCSdWElry"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQnSe-n1WCqv"},"outputs":[],"source":["# Boolean mask to find the orders that contain a price with multiple decimal points\n","multiple_decimal_mask = orderlines_df['unit_price'].str.count(\"\\.\") > 1\n","\n","# Apply the boolean mask to the orderlines DataFrame. This way we can find the order_id of all the affected orders.\n","corrupted_order_ids = orderlines_df.loc[multiple_decimal_mask, \"id_order\"]\n","\n","# Keep only the rows that do not have multiple decimal points\n","orderlines_df = orderlines_df.loc[~orderlines_df['id_order'].isin(corrupted_order_ids)].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-GLzXuV0Ioj"},"outputs":[],"source":["pre_delete = orderlines_df.shape[0]"]},{"cell_type":"code","source":["new_orderlines_df.shape[0] / pre_delete"],"metadata":{"id":"9XMQLNtREK3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2pd5t4110iz3"},"source":["We still have 216250 rows in orderlines to work with. This should be more than enough for our evaluation.\n","\n","Now that all of the 2 decimal point prices have been removed, let's try again to convert the column `unit_price` to the correct datatype."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4eDTAPs1HMZ"},"outputs":[],"source":["orderlines_df[\"unit_price\"] = pd.to_numeric(orderlines_df[\"unit_price\"])"]},{"cell_type":"markdown","metadata":{"id":"sBudIQLm6oJQ"},"source":["It worked perfectly"]},{"cell_type":"markdown","metadata":{"id":"1zGAzgIX8_-D"},"source":["# Challenge: Clean the `products` DataFrame\n","Now it's your turn. Use the lessons you learnt above and clean the products DataFrame. You don't have to copy exactly what we did. Think about the consequences of your actions, sometimes it is ok to delete rows, other times you may wish to come up with more creative solutions."]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"aRi6CuAF1BPT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtYqdFsa9KTS"},"outputs":[],"source":["# products.csv\n","url = \"https://drive.google.com/file/d/1afxwDXfl-7cQ_qLwyDitfcCx3u7WMvkU/view?usp=sharing\"\n","path = \"https://drive.google.com/uc?export=download&id=\"+url.split(\"/\")[-2]\n","products = pd.read_csv(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UW3zYoni9Vcy"},"outputs":[],"source":["products_df = products.copy()"]},{"cell_type":"markdown","metadata":{"id":"DyTfC-WY-IWe"},"source":["### Look for Duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXZXRvUa_GTW"},"outputs":[],"source":["products_df.duplicated().value_counts()"]},{"cell_type":"code","source":["products_df.drop_duplicates(inplace=True)"],"metadata":{"id":"yqZL1UlcD4Tk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting the product codes that have multiple different entries\n","duplicated_skus = products_df.loc[products_df.duplicated(subset='sku'), 'sku']\n","# Taking a look at all the versions of the repeated products\n","products_df.loc[products_df['sku'].isin(duplicated_skus)]"],"metadata":{"id":"oGxJ8-yyECtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping second occurance\n","products_df.drop_duplicates(subset='sku', inplace=True)"],"metadata":{"id":"xYQNKriCbf5M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTGqkqtm--ln"},"source":["### Look for Missing values\n"]},{"cell_type":"code","source":["# Checking for missing values in each column\n","products_df.isna().sum()"],"metadata":{"id":"z9gzrfCJC8h8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Only dropping rows where the price is missing\n","products_df.dropna(axis=0, subset='price', inplace=True)"],"metadata":{"id":"RriZHdDHFy-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the average price products were sold at from orderlines table\n","orderlines_prices = orderlines_df.groupby('sku').agg({'unit_price':'mean'})\n","\n","# Merge in to compare prices sold at with prices in products table\n","df_merged_prices = (\n","    products_df[['sku', 'price', 'promo_price', 'desc']]\n","    .merge(orderlines_prices,\n","    on='sku',\n","    how='outer')\n",")\n","# Sample can give us a random selection of rows to explore the data frame\n","df_merged_prices.sample(10)"],"metadata":{"id":"mKj8AbFrGaai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Double dot problem"],"metadata":{"id":"VTLk2_rb4qjn"}},{"cell_type":"code","source":["# Checks if there is more than 1 .\n","price_double_dot_mask = products_df['price'].str.count('\\.') > 1\n","# Get the proportion of trues and falses to see what percent is corrupted\n","price_double_dot_mask.value_counts(normalize=True)"],"metadata":{"id":"K6w71q9RMzdA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checks if there is more than 1 .\n","promo_double_dot_mask = products_df['promo_price'].str.count('\\.') > 1\n","# Get the proportion of trues and falses to see what percent is corrupted\n","promo_double_dot_mask.value_counts(normalize=True)"],"metadata":{"id":"bopymsgkNjzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking how much we would lose if we dropped both\n","(price_double_dot_mask | promo_double_dot_mask).value_counts(normalize=True)"],"metadata":{"id":"D8bhkr1QMGhu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### More than 2 numbers after the decimal"],"metadata":{"id":"2qT3p2vQ4tu3"}},{"cell_type":"code","source":["# Checking if there are 3 or more numbers after the last decimal\n","price_three_after_dec = products_df['price'].str.contains('\\.\\d{3,}$')\n","# Get the proportion of trues and falses to see what percent is corrupted\n","price_three_after_dec.value_counts(normalize=True)"],"metadata":{"id":"WLZBXICFOBtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking if there are 3 or more numbers after the last decimal\n","promo_three_after_dec = products_df['promo_price'].str.contains('\\.\\d{3,}$')\n","# Get the proportion of trues and falses to see what percent is corrupted\n","promo_three_after_dec.value_counts(normalize=True)"],"metadata":{"id":"Ji2yblTKC9Q1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking how much we would lose if we dropped both\n","(price_three_after_dec | promo_three_after_dec).value_counts(normalize=True)"],"metadata":{"id":"r2ZVVFhfN_Gd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4x6xeGDu_B0N"},"source":["### Check / Change Data types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3evkDaafvFUq"},"outputs":[],"source":["products.info()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1HzyoEpIvt28vm9S0i8JflTmA4FMhMDok","timestamp":1733244818732}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}